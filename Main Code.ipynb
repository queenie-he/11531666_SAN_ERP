{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845b1e91-4646-45c2-8423-834c0e5397e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# MSc Data Science - Extended Research Project\n",
    "# Technical Appendix: Python Code for Data Analysis\n",
    "#\n",
    "# This script contains the complete, reproducible Python code used for the\n",
    "# analysis of YouTube comments. It is designed to be self-contained and\n",
    "# heavily annotated to meet the reproducibility for future research.\n",
    "#\n",
    "# The code performs the following key tasks:\n",
    "# 1. Data Collection: Fetches top-level comments from YouTube videos related to\n",
    "#    a specific policy topic using the YouTube Data API v3.\n",
    "# 2. Text Preprocessing: Cleans, tokenizes, and stems the collected comments.\n",
    "# 3. Exploratory Analysis: Visualizes word frequencies, bigram networks, and\n",
    "#    performs Topic Modeling using Non-negative Matrix Factorization (NMF).\n",
    "# 4. Sentiment Analysis: Compares multiple sentiment models (VADER, TextBlob,\n",
    "#    BERT) and trains a custom Logistic Regression classifier.\n",
    "# 5. Model Comparison: Evaluates the performance of all models to identify the\n",
    "#    best-performing one for a final deep-dive analysis.\n",
    "#\n",
    "# To run this script:\n",
    "# - First, set up the environment and dependencies. The recommended method is\n",
    "#   to create a `requirements.txt` file by running `pip freeze > requirements.txt`\n",
    "#   in your virtual environment. This file should be included in your submission\n",
    "#   along with this script.\n",
    "# - Ensure all required libraries are installed by running:\n",
    "#   pip install -r requirements.txt\n",
    "# - Securely provide your YouTube Data API key. This script expects the key to\n",
    "#   be set as an environment variable named 'YOUTUBE_API_KEY'. This prevents\n",
    "#   your key from being exposed in a public repository.\n",
    "# - For the Logistic Regression section, this script attempts to load a\n",
    "#   'manual_checked_sample.csv' file for training data. If this file is not\n",
    "#   found, the script will gracefully skip that part of the analysis.\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 0. Import Required Libraries ---------------------------------------------\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from collections import defaultdict, Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from matplotlib import cm\n",
    "from textblob import TextBlob\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Download NLTK data if not already present\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading NLTK stopwords...\")\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# --- 1. Data Collection -------------------------------------------------------\n",
    "\n",
    "def get_youtube_api_client():\n",
    "    \"\"\"\n",
    "    Initializes and returns a YouTube Data API client, fetching the API key\n",
    "    from an environment variable for security and reproducibility.\n",
    "    \"\"\"\n",
    "    api_key = os.getenv('YOUTUBE_API_KEY')\n",
    "    if not api_key:\n",
    "        print(\"Error: YOUTUBE_API_KEY environment variable not set.\")\n",
    "        print(\"Please set this variable with your YouTube Data API key.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        youtube_client = build('youtube', 'v3', developerKey=api_key)\n",
    "        return youtube_client\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing YouTube API client: {e}\")\n",
    "        return None\n",
    "\n",
    "# Helper Function: Search for YouTube Videos\n",
    "def search_videos(youtube_client, query, max_results=5):\n",
    "    \"\"\"\n",
    "    Searches YouTube for videos matching a given query, focusing on UK region and English language.\n",
    "    Returns a list of dictionaries with video details.\n",
    "    \"\"\"\n",
    "    if not youtube_client:\n",
    "        return []\n",
    "\n",
    "    video_details = []\n",
    "    try:\n",
    "        search_response = youtube_client.search().list(\n",
    "            q=query,\n",
    "            type=\"video\",\n",
    "            part=\"id, snippet\",\n",
    "            maxResults=max_results,\n",
    "            relevanceLanguage=\"en\",\n",
    "            regionCode=\"GB\"\n",
    "        ).execute()\n",
    "\n",
    "        for item in search_response.get(\"items\", []):\n",
    "            if 'id' in item and 'videoId' in item['id']:\n",
    "                video_details.append({\n",
    "                    \"video_id\": item['id']['videoId'],\n",
    "                    \"title\": item['snippet']['title'],\n",
    "                    \"description\": item['snippet']['description'],\n",
    "                    \"published_at\": item['snippet']['publishedAt']\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: Skipping invalid item (no video ID) for query '{query}'.\")\n",
    "    except HttpError as e:\n",
    "        print(f\"API Error during search for '{query}': {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error during search for '{query}': {str(e)}\")\n",
    "    return video_details\n",
    "\n",
    "# Helper Function: Retrieve Video Comments\n",
    "def get_video_comments(youtube_client, video_id, max_comments=2000):\n",
    "    \"\"\"\n",
    "    Fetches top-level comments from a YouTube video, handling pagination.\n",
    "    Returns a list of dictionaries with comment details.\n",
    "    \"\"\"\n",
    "    if not youtube_client:\n",
    "        return []\n",
    "\n",
    "    comments = []\n",
    "    next_page_token = None\n",
    "    try:\n",
    "        while len(comments) < max_comments:\n",
    "            response = youtube_client.commentThreads().list(\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=min(100, max_comments - len(comments)),\n",
    "                pageToken=next_page_token,\n",
    "                textFormat=\"plainText\"\n",
    "            ).execute()\n",
    "\n",
    "            for item in response.get(\"items\", []):\n",
    "                top_comment = item[\"snippet\"][\"topLevelComment\"][\"snippet\"]\n",
    "                comments.append({\n",
    "                    \"video_id\": video_id,\n",
    "                    \"comment_id\": item[\"id\"],\n",
    "                    \"text\": top_comment[\"textDisplay\"],\n",
    "                    \"author\": top_comment[\"authorDisplayName\"],\n",
    "                    \"published_at\": top_comment[\"publishedAt\"],\n",
    "                    \"like_count\": top_comment[\"likeCount\"]\n",
    "                })\n",
    "\n",
    "            if len(comments) >= max_comments:\n",
    "                break\n",
    "\n",
    "            next_page_token = response.get(\"nextPageToken\")\n",
    "            if not next_page_token:\n",
    "                break\n",
    "    except HttpError as e:\n",
    "        if \"commentsDisabled\" in str(e):\n",
    "            print(f\"Warning: Comments are disabled for video {video_id}. Skipping.\")\n",
    "        else:\n",
    "            print(f\"API Error for video {video_id}: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error for video {video_id}: {str(e)}\")\n",
    "    return comments\n",
    "\n",
    "# Main workflow to collect all comments\n",
    "def collect_data(keywords):\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the data collection process.\n",
    "    \"\"\"\n",
    "    print(\"--- Starting Data Collection ---\")\n",
    "    youtube_client = get_youtube_api_client()\n",
    "    if not youtube_client:\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "    all_comments = []\n",
    "    all_video_details = []\n",
    "\n",
    "    for keyword in keywords:\n",
    "        print(f\"\\nSearching for videos with keyword: '{keyword}'\")\n",
    "        video_details = search_videos(youtube_client, query=keyword, max_results=5)\n",
    "        all_video_details.extend(video_details)\n",
    "        \n",
    "        if not video_details:\n",
    "            print(\"No videos found for this keyword.\")\n",
    "        else:\n",
    "            print(f\"\\n--- Displaying Video Metadata for keyword: '{keyword}' ---\")\n",
    "            for detail in video_details:\n",
    "                print(f\"Video ID: {detail['video_id']}\")\n",
    "                print(f\"Title: {detail['title']}\")\n",
    "                print(f\"Published: {detail['published_at']}\")\n",
    "                print(f\"Description: {detail['description'][:100]}...\")\n",
    "                print(\"-\" * 20)\n",
    "        \n",
    "        for detail in video_details:\n",
    "            video_id = detail['video_id']\n",
    "            print(f\"Collecting comments from video: {video_id}\")\n",
    "            video_comments = get_video_comments(youtube_client, video_id=video_id, max_comments=2000)\n",
    "            all_comments.extend(video_comments)\n",
    "\n",
    "    df_videos = pd.DataFrame(all_video_details)\n",
    "    df_raw_comments = pd.DataFrame(all_comments)\n",
    "    return df_videos, df_raw_comments\n",
    "\n",
    "# --- Main execution block -----------------------------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define Search Keywords\n",
    "    KEYWORDS = [\n",
    "        'maths to 18', 'Sunak education policy', 'post-16 mathematics reform',\n",
    "        \"sunak math policy\", \"math until 18\", \"math to 18\",\n",
    "        \"extended math education\", \"compulsory math 18\",\n",
    "        \"sunak education reform\", \"uk math extension\",\n",
    "        \"british math curriculum to 18\", \"England math requirement sunak\",\n",
    "        \"Sunak school reform\"\n",
    "    ]\n",
    "    \n",
    "    # Collect and save data\n",
    "    df_videos, df_raw_comments = collect_data(KEYWORDS)\n",
    "\n",
    "    if not df_raw_comments.empty:\n",
    "        # Convert raw comments to a DataFrame\n",
    "        df_raw_comments = pd.DataFrame(df_raw_comments)\n",
    "        \n",
    "        # Save raw data to a platform-agnostic path\n",
    "        output_path_raw_comments = os.path.join(\"data\", \"youtube_comments_metadata_raw.csv\")\n",
    "        os.makedirs(\"data\", exist_ok=True)\n",
    "        df_raw_comments.to_csv(output_path_raw_comments, index=False)\n",
    "        print(f\"\\nRaw collected comments (before filtering/cleaning) saved to: {output_path_raw_comments}\")\n",
    "\n",
    "        # Remove duplicates and prepare the main DataFrame\n",
    "        df_main = df_raw_comments.drop_duplicates(subset=[\"text\"]).copy()\n",
    "        print(f\"Removed duplicates. Retained {len(df_main)} unique comments for analysis.\")\n",
    "\n",
    "        # Filter comments by date range\n",
    "        print(\"\\n--- Filtering comments by date range: 2023.01.01 to 2023.12.31 ---\")\n",
    "        df_main['published_at'] = pd.to_datetime(df_main['published_at']).dt.tz_convert('UTC')\n",
    "        start_date = pd.to_datetime('2023-01-01', utc=True)\n",
    "        end_date = pd.to_datetime('2023-12-31', utc=True)\n",
    "        df_main = df_main[(df_main['published_at'] >= start_date) & (df_main['published_at'] <= end_date)].reset_index(drop=True)\n",
    "        print(f\"Retained {len(df_main)} comments after date filtering.\")\n",
    "        \n",
    "        # Save filtered data\n",
    "        output_path_filtered_comments = os.path.join(\"data\", \"youtube_comments_2023.csv\")\n",
    "        df_main.to_csv(output_path_filtered_comments, index=False)\n",
    "        print(f\"Filtered comments for 2023 saved to: {output_path_filtered_comments}\")\n",
    "        \n",
    "        # --- 2. Text Preprocessing and Configuration ----------------------------------\n",
    "\n",
    "        # Configuration dictionary for all analysis steps\n",
    "        config = {\n",
    "            'target_terms': [\"sunak\", \"rishi\", \"math\", \"maths\", \"mathematics\", \"A-level\", \"post-16\"],\n",
    "            'stopwords': set(stopwords.words('english')) | {'video', 'comment', 'like', 'subscribe', 'youtube', 'http', 'www', 't', 's'},\n",
    "            'bigram_min_count': 10,\n",
    "            'top_n_bigrams': 30,\n",
    "            'top_n_words': 20,\n",
    "            'nmf_topic_range': range(3, 8),\n",
    "            'sentiment_cutoffs': {'positive': 0.05, 'negative': -0.05},\n",
    "            'stems': {},  # Placeholder for custom stemmed lexicon\n",
    "            'bert_batch_size': 32\n",
    "        }\n",
    "\n",
    "        # Text Preprocessing Functions\n",
    "        def preprocess_text(text):\n",
    "            \"\"\"\n",
    "            Cleans and tokenizes text by removing special characters, URLs, and stopwords, then performs stemming.\n",
    "            \"\"\"\n",
    "            stemmer = SnowballStemmer('english')\n",
    "            if not isinstance(text, str):\n",
    "                return []\n",
    "            text = re.sub(r'http\\S+|@\\w+|#\\w+', '', text)\n",
    "            text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            tokens = [stemmer.stem(t.lower()) for t in word_tokenize(text) if t.isalpha()]\n",
    "            tokens = [t for t in tokens if t not in config['stopwords'] and len(t) > 2]\n",
    "            return tokens\n",
    "\n",
    "        def clean_text_base(text):\n",
    "            \"\"\"Basic cleaning for non-stemmed text, used for models like BERT.\"\"\"\n",
    "            if not isinstance(text, str):\n",
    "                return \"\"\n",
    "            text = re.sub(r'http\\S+|@\\w+|#\\w+', '', text)\n",
    "            text = re.sub(r'[^A-Za-z0-9\\s.,?!]', '', text)\n",
    "            return text.strip()\n",
    "\n",
    "        # Apply preprocessing to the main dataset\n",
    "        if not df_main.empty:\n",
    "            df_main['raw_comment'] = df_main['text'].astype(str).str.strip()\n",
    "            df_main = df_main[df_main['raw_comment'].str.contains(\n",
    "                r\"\\b(?:\" + \"|\".join(map(re.escape, config['target_terms'])) + r\")\\b\", case=False, na=False\n",
    "            )].reset_index(drop=True)\n",
    "\n",
    "            df_main['tokens'] = df_main['raw_comment'].apply(preprocess_text)\n",
    "            df_main['clean_text'] = df_main['tokens'].apply(lambda tk: \" \".join(tk))\n",
    "            df_main['clean_text_base'] = df_main['raw_comment'].apply(clean_text_base)\n",
    "            df_main = df_main[df_main['clean_text'].str.len() > 0].reset_index(drop=True)\n",
    "            print(f\"\\n{len(df_main)} comments after preprocessing and filtering.\")\n",
    "        else:\n",
    "            print(\"No data to preprocess.\")\n",
    "            # Create empty dataframe to avoid errors in subsequent steps\n",
    "            df_main = pd.DataFrame(columns=['raw_comment', 'tokens', 'clean_text', 'clean_text_base'])\n",
    "\n",
    "        # --- 3. Word Frequency & Bigram Network ---------------------------------------\n",
    "\n",
    "        # Function to analyze and plot top N most frequent words\n",
    "        def analyze_word_frequency(df, n_top_words):\n",
    "            \"\"\"Generates and plots top N most frequent words from preprocessed tokens.\"\"\"\n",
    "            print(\"\\n--- Word Frequency Analysis ---\")\n",
    "            if df.empty:\n",
    "                print(\"No data for word frequency analysis.\")\n",
    "                return None\n",
    "            all_tokens = [token for sublist in df['tokens'] for token in sublist]\n",
    "            word_counts = Counter(all_tokens)\n",
    "            top_words_df = pd.DataFrame(word_counts.items(), columns=['word', 'count']).sort_values('count', ascending=False).head(n_top_words)\n",
    "\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x='count', y='word', data=top_words_df, palette='viridis', hue='word', legend=False)\n",
    "            for i, (cnt, wd) in enumerate(zip(top_words_df['count'], top_words_df['word'])):\n",
    "                plt.text(cnt + 1, i, str(cnt), va='center')\n",
    "            plt.title(f\"Top {n_top_words} Most Frequent Words\", pad=15)\n",
    "            plt.xlabel('Frequency')\n",
    "            plt.ylabel('Word')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            return top_words_df\n",
    "\n",
    "        # Function to generate and plot bigram network\n",
    "        def generate_bigrams_detailed(df, top_n_bigrams):\n",
    "            \"\"\"Generates a network graph of the most frequent bigrams.\"\"\"\n",
    "            print(\"\\n--- Bigram Network Analysis ---\")\n",
    "            if df.empty:\n",
    "                print(\"No data for bigram analysis.\")\n",
    "                return None\n",
    "            bigram_counts = defaultdict(int)\n",
    "            for tokens in df['tokens']:\n",
    "                for i in range(len(tokens) - 1):\n",
    "                    bigram_counts[(tokens[i], tokens[i+1])] += 1\n",
    "\n",
    "            top_bigrams = dict(sorted(bigram_counts.items(), key=lambda x: x[1], reverse=True)[:top_n_bigrams])\n",
    "\n",
    "            G = nx.Graph()\n",
    "            for (w1, w2), cnt in top_bigrams.items():\n",
    "                G.add_edge(w1, w2, weight=cnt)\n",
    "\n",
    "            pos = nx.spring_layout(G, k=0.8, seed=42, iterations=50)\n",
    "            max_weight = max(top_bigrams.values()) if top_bigrams else 1\n",
    "            node_sizes = [300 + 100 * G.degree(n) for n in G.nodes()]\n",
    "            edge_widths = [5 * G[u][v]['weight'] / max_weight for u, v in G.edges()]\n",
    "\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=cm.Paired(np.linspace(0, 1, len(G))))\n",
    "            nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color='gray', alpha=0.7)\n",
    "            nx.draw_networkx_labels(G, pos, font_size=12, font_weight='bold')\n",
    "            edge_labels = {(u, v): d['weight'] for u, v, d in G.edges(data=True)}\n",
    "            nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=10, label_pos=0.5)\n",
    "\n",
    "            plt.title(f\"Top {top_n_bigrams} Frequent Bigrams\", pad=20)\n",
    "            plt.axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            return top_bigrams\n",
    "\n",
    "        if not df_main.empty:\n",
    "            word_freq = analyze_word_frequency(df_main, config['top_n_words'])\n",
    "            bigrams = generate_bigrams_detailed(df_main, config['top_n_bigrams'])\n",
    "\n",
    "        # --- 4. Topic Modeling & Coherence Sweep --------------------------------------\n",
    "\n",
    "        # Function to perform NMF and coherence score sweep\n",
    "        def perform_nmf_coherence_sweep(df, topic_range):\n",
    "            \"\"\"\n",
    "            Uses TF-IDF to prepare data for NMF and calculates coherence scores for different\n",
    "            numbers of topics (k) to find the optimal number of topics.\n",
    "            \"\"\"\n",
    "            print(\"\\n--- Topic Modeling & Coherence Sweep ---\")\n",
    "            if df.empty:\n",
    "                print(\"No data for topic modeling.\")\n",
    "                return None, None, None\n",
    "\n",
    "            tfidf_vec = TfidfVectorizer(min_df=5, max_df=0.3)\n",
    "            tfidf_matrix = tfidf_vec.fit_transform(df['clean_text'])\n",
    "            vocab = tfidf_vec.get_feature_names_out()\n",
    "\n",
    "            # Create a frequency-based vocabulary for coherence scoring\n",
    "            count_vec = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, token_pattern=None)\n",
    "            count_matrix = count_vec.fit_transform(df['tokens'])\n",
    "            freq_vocab = count_vec.get_feature_names_out()\n",
    "            word_counts = count_matrix.sum(axis=0).A1\n",
    "            freq_df = pd.DataFrame({'word': freq_vocab, 'count': word_counts}).sort_values('count', ascending=False).reset_index(drop=True)\n",
    "\n",
    "            ks = list(topic_range)\n",
    "            coherence_scores = []\n",
    "\n",
    "            for k in ks:\n",
    "                # --- NOTE ON WARNINGS ---\n",
    "                # The `ConvergenceWarning` from NMF often indicates that the model\n",
    "                # did not fully converge within the set number of iterations. To address\n",
    "                # this, you might increase the `max_iter` parameter in your NMF\n",
    "                # model. For example, `max_iter=1000` is used here to improve convergence.\n",
    "                nmf = NMF(n_components=k, random_state=42, max_iter=1000)\n",
    "                nmf.fit(tfidf_matrix)\n",
    "                topics = [[vocab[i] for i in comp.argsort()[:-11:-1]] for comp in nmf.components_]\n",
    "                \n",
    "                # Calculate a simple coherence score based on top words\n",
    "                coh = np.mean([len(set(t) & set(freq_df['word'][:100])) for t in topics])\n",
    "                coherence_scores.append(coh)\n",
    "                print(f\"k={k}, coherence: {coh:.3f}\")\n",
    "\n",
    "            # Plot coherence scores\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.plot(ks, coherence_scores, 'o-', linewidth=2, markersize=6)\n",
    "            for k, sc in zip(ks, coherence_scores):\n",
    "                plt.text(k, sc + 0.005, f\"{sc:.2f}\", ha='center')\n",
    "            plt.title('Coherence Score vs Number of Topics (k)', pad=15)\n",
    "            plt.xlabel('Number of Topics (k)')\n",
    "            plt.ylabel('Coherence Score')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Logic to select k based on user's preference\n",
    "            coherence_df = pd.DataFrame({'k': ks, 'score': coherence_scores}).sort_values(by='score', ascending=False)\n",
    "            \n",
    "            best_k = None\n",
    "            # Check if the highest scoring k is > 5\n",
    "            if coherence_df.iloc[0]['k'] > 5:\n",
    "                best_k = int(coherence_df.iloc[0]['k'])\n",
    "                print(f\"Selecting the highest scoring k which is > 5: {best_k}\")\n",
    "            else:\n",
    "                # If not, find the next highest scoring k > 5\n",
    "                k_options = coherence_df[coherence_df['k'] > 5]\n",
    "                if not k_options.empty:\n",
    "                    best_k = int(k_options.iloc[0]['k'])\n",
    "                    print(f\"The best k <= 5. Selecting the next highest scoring k > 5: {best_k}\")\n",
    "                else:\n",
    "                    # Fallback to the absolute highest scoring k if no k > 5 is available\n",
    "                    best_k = int(coherence_df.iloc[0]['k'])\n",
    "                    print(f\"No k > 5 found in the range. Selecting best overall k: {best_k}\")\n",
    "\n",
    "            return best_k, tfidf_matrix, vocab\n",
    "\n",
    "        # Execute the topic modeling analysis\n",
    "        if not df_main.empty:\n",
    "            best_k, tfidf_matrix, vocab = perform_nmf_coherence_sweep(df_main, config['nmf_topic_range'])\n",
    "        else:\n",
    "            best_k = None\n",
    "            tfidf_matrix = None\n",
    "            vocab = None\n",
    "\n",
    "        # --- 5. Final NMF & Topic Visualizations --------------------------------------\n",
    "\n",
    "        # Function to get human-readable topic names\n",
    "        def get_topic_names(nmf_model, vocab, n_top_words):\n",
    "            \"\"\"Assigns human-readable names to topics based on their top terms.\"\"\"\n",
    "            topic_names = {}\n",
    "            for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "                top_words_idx = topic.argsort()[:-n_top_words - 1:-1]\n",
    "                top_words = [vocab[i] for i in top_words_idx]\n",
    "                topic_names[topic_idx] = f\"Topic {topic_idx}: {', '.join(top_words[:3])}\"\n",
    "            return topic_names\n",
    "\n",
    "        # Main workflow for final NMF and visualizations\n",
    "        if best_k and not df_main.empty:\n",
    "            print(\"\\n--- Final NMF & Topic Visualizations ---\")\n",
    "            nmf_final = NMF(n_components=best_k, random_state=42, max_iter=1000)\n",
    "            topic_matrix = nmf_final.fit_transform(tfidf_matrix)\n",
    "            df_main['dominant_topic'] = topic_matrix.argmax(axis=1)\n",
    "\n",
    "            topic_names = get_topic_names(nmf_final, vocab, 10)\n",
    "            df_main['dominant_topic_name'] = df_main['dominant_topic'].map(topic_names)\n",
    "\n",
    "            # 5a) Top-terms bar chart per topic\n",
    "            for t in range(best_k):\n",
    "                comp = nmf_final.components_[t]\n",
    "                top_idx = comp.argsort()[-10:]\n",
    "                terms = vocab[top_idx][::-1]\n",
    "                weights = comp[top_idx][::-1]\n",
    "                plt.figure(figsize=(6, 4))\n",
    "                sns.barplot(x=weights, y=terms, hue=terms, legend=False, palette='viridis')\n",
    "                for i, w in enumerate(weights):\n",
    "                    plt.text(w + 0.001, i, f\"{w:.2f}\", va='center')\n",
    "                plt.title(f\"{topic_names[t]} : Top 10 Terms\", pad=10)\n",
    "                plt.xlabel(\"Weight\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "            # Print all top terms for each topic\n",
    "            print(\"\\n--- Top 10 Terms for Each Topic ---\")\n",
    "            for t in range(best_k):\n",
    "                comp = nmf_final.components_[t]\n",
    "                top_idx = comp.argsort()[-10:]\n",
    "                terms = [vocab[i] for i in top_idx][::-1]\n",
    "                print(f\"{topic_names[t]}: {', '.join(terms)}\")\n",
    "\n",
    "\n",
    "            # 5b) Comments per topic bar chart\n",
    "            counts = df_main['dominant_topic_name'].value_counts().sort_index()\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.barplot(x=counts.index, y=counts.values, palette='tab10', hue=counts.index, legend=False)\n",
    "            for i, c in enumerate(counts):\n",
    "                plt.text(i, c + 5, str(c), ha='center')\n",
    "            plt.title(\"Number of Comments per Topic\", pad=10)\n",
    "            plt.xlabel(\"Topic\")\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # 5c) WordCloud for each topic\n",
    "            for t in range(best_k):\n",
    "                comp = nmf_final.components_[t]\n",
    "                top_idx = comp.argsort()[-50:]\n",
    "                freqs = {vocab[i]: float(comp[i]) for i in top_idx}\n",
    "                wc = WordCloud(width=800, height=400, background_color='white', prefer_horizontal=0.9).generate_from_frequencies(freqs)\n",
    "                plt.figure(figsize=(10, 5))\n",
    "                plt.imshow(wc, interpolation='bilinear')\n",
    "                plt.axis('off')\n",
    "                plt.title(f'Word-Cloud for {topic_names[t]}', pad=15)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "        # --- 6. Sentiment Analysis with Multiple Models -------------------------------\n",
    "\n",
    "        class EnhancedSentimentAnalyzer:\n",
    "            \"\"\"\n",
    "            A class to encapsulate and compare multiple sentiment analysis models.\n",
    "            Includes VADER, TextBlob, and a BERT-based pipeline.\n",
    "            \"\"\"\n",
    "            def __init__(self):\n",
    "                self.vader = SentimentIntensityAnalyzer()\n",
    "                self.vader.lexicon.update(config['stems'])\n",
    "                self.bert_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(self.bert_name)\n",
    "                self.model = AutoModelForSequenceClassification.from_pretrained(self.bert_name)\n",
    "                self.bert_pipeline = pipeline(\n",
    "                    \"sentiment-analysis\",\n",
    "                    model=self.model,\n",
    "                    tokenizer=self.tokenizer,\n",
    "                    device=0 if torch.cuda.is_available() else -1\n",
    "                )\n",
    "                self.max_length = self.tokenizer.model_max_length\n",
    "\n",
    "            def truncate_text(self, text):\n",
    "                \"\"\"Truncates text to fit model's maximum sequence length.\"\"\"\n",
    "                tokens = self.tokenizer.tokenize(str(text))\n",
    "                if len(tokens) > self.max_length - 2:\n",
    "                    tokens = tokens[:self.max_length - 2]\n",
    "                return self.tokenizer.convert_tokens_to_string(tokens)\n",
    "\n",
    "            def vader_score(self, text):\n",
    "                \"\"\"Generates sentiment score using VADER.\"\"\"\n",
    "                score = self.vader.polarity_scores(text)['compound']\n",
    "                if score > config['sentiment_cutoffs']['positive']:\n",
    "                    return {'sentiment': 'positive', 'score': score}\n",
    "                elif score < config['sentiment_cutoffs']['negative']:\n",
    "                    return {'sentiment': 'negative', 'score': score}\n",
    "                return {'sentiment': 'neutral', 'score': 0.0}\n",
    "\n",
    "            def textblob_score(self, text):\n",
    "                \"\"\"Generates sentiment score using TextBlob.\"\"\"\n",
    "                polarity = TextBlob(str(text)).sentiment.polarity\n",
    "                if polarity > 0.1:\n",
    "                    return {'sentiment': 'positive', 'score': polarity}\n",
    "                elif polarity < -0.1:\n",
    "                    return {'sentiment': 'negative', 'score': polarity}\n",
    "                return {'sentiment': 'neutral', 'score': 0.0}\n",
    "\n",
    "            def bert_score_batch(self, texts):\n",
    "                \"\"\"Batch processing for BERT sentiment analysis to improve efficiency.\"\"\"\n",
    "                results = []\n",
    "                for i in range(0, len(texts), config['bert_batch_size']):\n",
    "                    batch = texts[i:i + config['bert_batch_size']]\n",
    "                    processed = [self.truncate_text(t) for t in batch]\n",
    "                    # The following code handles the Tokenizer truncation warning without disruption\n",
    "                    with pd.option_context('mode.chained_assignment', None):\n",
    "                      batch_results = self.bert_pipeline(processed, truncation=True, max_length=self.max_length)\n",
    "                    \n",
    "                    for res in batch_results:\n",
    "                        sentiment = 'positive' if res['label'] == 'POSITIVE' else 'negative'\n",
    "                        score = res['score'] if sentiment == 'positive' else -res['score']\n",
    "                        if abs(score) < 0.3:\n",
    "                            sentiment = 'neutral'\n",
    "                        results.append({'sentiment': sentiment, 'score': score})\n",
    "                return results\n",
    "\n",
    "        # Main workflow for sentiment analysis and model comparison\n",
    "        if not df_main.empty:\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"PART 3: SENTIMENT ANALYSIS WITH MODEL COMPARISON\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "            analyzer = EnhancedSentimentAnalyzer()\n",
    "\n",
    "            # Process main dataset with all models\n",
    "            print(\"Running VADER sentiment analysis...\")\n",
    "            df_main['vader'] = df_main.clean_text_base.apply(analyzer.vader_score)\n",
    "            print(\"Running TextBlob sentiment analysis...\")\n",
    "            df_main['textblob'] = df_main.clean_text_base.apply(analyzer.textblob_score)\n",
    "            print(\"Running BERT sentiment analysis...\")\n",
    "            bert_results = analyzer.bert_score_batch(df_main.raw_comment.tolist())\n",
    "            df_main['bert'] = bert_results\n",
    "\n",
    "            # --- Train Logistic Regression model using the provided manual data ---\n",
    "            try:\n",
    "                # Load the manually checked sample dataset for training\n",
    "                print(\"\\nLoading and training on your manual_checked_sample.csv file...\")\n",
    "                df_manual = pd.read_csv('manual_checked_sample.csv')\n",
    "                \n",
    "                # Ensure the 'clean_comment' and 'clean_comment_base' columns are treated as string types before any operations\n",
    "                for col in ['clean_comment', 'clean_comment_base']:\n",
    "                    if col in df_manual.columns:\n",
    "                        df_manual[col] = df_manual[col].astype(str)\n",
    "\n",
    "                df_manual['manual_label'] = df_manual['manual_label'].str.lower()\n",
    "                df_manual = df_manual[df_manual['manual_label'].isin(['positive', 'negative', 'neutral'])]\n",
    "                print(f\"Loaded {len(df_manual)} comments from manual_checked_sample.csv for training.\")\n",
    "                \n",
    "                if not df_manual.empty:\n",
    "                    # Apply the same text cleaning as the main dataset\n",
    "                    if 'clean_comment' in df_manual.columns:\n",
    "                        X_train_data = df_manual['clean_comment'].fillna('').apply(clean_text_base)\n",
    "                    else:\n",
    "                        raise KeyError(\"Column 'clean_comment' not found in manual_check_sample.csv\")\n",
    "                    \n",
    "                    label_mapping = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "                    y_train_labels = df_manual['manual_label'].map(label_mapping)\n",
    "                    \n",
    "                    X_train, X_test, y_train, y_test = train_test_split(\n",
    "                        X_train_data,\n",
    "                        y_train_labels,\n",
    "                        test_size=0.2, random_state=42\n",
    "                    )\n",
    "                    \n",
    "                    # --- Hyperparameter Tuning for Logistic Regression ---\n",
    "                    print(\"\\nPerforming hyperparameter tuning with GridSearchCV...\")\n",
    "                    \n",
    "                    # Define the pipeline with a vectorizer and a classifier\n",
    "                    pipeline_logreg = Pipeline([\n",
    "                        ('tfidf', TfidfVectorizer()),\n",
    "                        ('clf', LogisticRegression(max_iter=1000, solver='liblinear')) # Using liblinear for L1/L2 regularization\n",
    "                    ])\n",
    "                    \n",
    "                    # Define the parameter grid to search over\n",
    "                    param_grid = {\n",
    "                        'tfidf__ngram_range': [(1, 1), (1, 2)], # Uni-grams and bigrams\n",
    "                        'clf__C': [0.1, 1, 10, 100] # Inverse of regularization strength\n",
    "                    }\n",
    "                    \n",
    "                    # Instantiate GridSearchCV with the pipeline and parameter grid\n",
    "                    grid_search = GridSearchCV(\n",
    "                        pipeline_logreg,\n",
    "                        param_grid,\n",
    "                        cv=5, # 5-fold cross-validation\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1 # Use all available cores\n",
    "                    )\n",
    "                    \n",
    "                    grid_search.fit(X_train, y_train)\n",
    "\n",
    "                    print(\"\\nBest parameters found: \", grid_search.best_params_)\n",
    "                    best_logreg_model = grid_search.best_estimator_\n",
    "\n",
    "                    print(\"\\nOptimized Logistic Regression Performance on Test Set:\")\n",
    "                    # --- NOTE ON WARNINGS ---\n",
    "                    # The `UndefinedMetricWarning` often occurs when a model's\n",
    "                    # predictions for a specific class (e.g., 'neutral') are\n",
    "                    # zero, especially in a small or imbalanced test set.\n",
    "                    # This warning can be safely ignored if you understand the\n",
    "                    # reason (e.g., your test set did not contain any 'neutral'\n",
    "                    # comments for the model to predict).\n",
    "                    y_pred = best_logreg_model.predict(X_test)\n",
    "                    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "                    # Apply the trained Logistic Regression model to the full dataset\n",
    "                    df_main['logreg_pred'] = best_logreg_model.predict(df_main['clean_text_base'].fillna(''))\n",
    "                    df_main['logreg_prob'] = best_logreg_model.predict_proba(df_main['clean_text_base'].fillna('')).max(axis=1)\n",
    "                else:\n",
    "                    print(\"Not enough manually checked data to train a Logistic Regression model.\")\n",
    "                    df_main['logreg_pred'] = np.nan\n",
    "                    df_main['logreg_prob'] = np.nan\n",
    "                \n",
    "            except FileNotFoundError:\n",
    "                print(\"\\n'manual_check_sample.csv' not found. Skipping Logistic Regression training and evaluation.\")\n",
    "                df_main['logreg_pred'] = np.nan\n",
    "                df_main['logreg_prob'] = np.nan\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError training Logistic Regression: {e}. Skipping.\")\n",
    "                df_main['logreg_pred'] = np.nan\n",
    "                df_main['logreg_prob'] = np.nan\n",
    "\n",
    "            # Define models and unpack results\n",
    "            models = ['vader', 'textblob', 'bert']\n",
    "            if 'logreg_pred' in df_main.columns:\n",
    "                models.append('logreg')\n",
    "\n",
    "            for model_name in models:\n",
    "                if model_name == 'logreg':\n",
    "                    logreg_mapping = {1: 'positive', 0: 'neutral', -1: 'negative'}\n",
    "                    df_main[f'{model_name}_sentiment'] = df_main['logreg_pred'].map(logreg_mapping)\n",
    "                    df_main[f'{model_name}_score'] = df_main['logreg_prob']\n",
    "                else:\n",
    "                    df_main[f'{model_name}_sentiment'] = df_main[model_name].apply(lambda x: x['sentiment'])\n",
    "                    df_main[f'{model_name}_score'] = df_main[model_name].apply(lambda x: x['score'])\n",
    "\n",
    "            # --- 7. Model Comparison Framework ----------------------------------------\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(\"MODEL COMPARISON ANALYSIS\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "            # Create a consensus sentiment for comparison\n",
    "            df_main['consensus_sentiment'] = df_main.apply(\n",
    "                lambda row: max(\n",
    "                    [row.get(f'{m}_sentiment') for m in models if pd.notna(row.get(f'{m}_sentiment'))],\n",
    "                    key=Counter([row.get(f'{m}_sentiment') for m in models if pd.notna(row.get(f'{m}_sentiment'))]).get\n",
    "                ) if any(pd.notna(row.get(f'{m}_sentiment')) for m in models) else 'neutral',\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "            model_metrics = {}\n",
    "            for model in models:\n",
    "                df_temp = df_main.dropna(subset=[f'{model}_sentiment'])\n",
    "                if df_temp.empty:\n",
    "                    continue\n",
    "                consensus_agreement = (df_temp[f'{model}_sentiment'] == df_temp['consensus_sentiment']).mean()\n",
    "                avg_confidence = df_temp[f'{model}_score'].abs().mean()\n",
    "                pred_variance = df_temp.groupby('dominant_topic_name')[f'{model}_score'].var().mean() if 'dominant_topic_name' in df_temp.columns else np.nan\n",
    "                \n",
    "                pairwise_agreements = [\n",
    "                    (df_temp[f'{model}_sentiment'] == df_temp[f'{other_model}_sentiment']).mean()\n",
    "                    for other_model in models if other_model != model\n",
    "                ]\n",
    "                avg_pairwise_agreement = np.mean(pairwise_agreements) if pairwise_agreements else 0.0\n",
    "\n",
    "                model_metrics[model] = {\n",
    "                    'consensus_agreement': consensus_agreement,\n",
    "                    'avg_confidence': avg_confidence,\n",
    "                    'prediction_variance': pred_variance,\n",
    "                    'avg_pairwise_agreement': avg_pairwise_agreement,\n",
    "                }\n",
    "\n",
    "            metrics_df = pd.DataFrame(model_metrics).T\n",
    "\n",
    "            # Normalize metrics for comparison\n",
    "            normalized = (metrics_df - metrics_df.min()) / (metrics_df.max() - metrics_df.min())\n",
    "            normalized['prediction_variance'] = 1 - normalized['prediction_variance']\n",
    "            normalized['overall_score'] = normalized.mean(axis=1)\n",
    "            normalized = normalized.sort_values('overall_score', ascending=False)\n",
    "\n",
    "            print(\"\\nModel Performance Metrics:\")\n",
    "            print(metrics_df.round(4))\n",
    "            print(\"\\nNormalized Metrics (0-1 scale) with Overall Score:\")\n",
    "            print(normalized.round(4))\n",
    "            \n",
    "            # Visualize overall performance\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.barplot(x=normalized.index, y='overall_score', data=normalized, palette='viridis', hue=normalized.index, legend=False)\n",
    "            for i, score in enumerate(normalized['overall_score']):\n",
    "                plt.text(i, score + 0.02, f\"{score:.2f}\", ha='center', fontweight='bold')\n",
    "            plt.title(\"Model Overall Performance Comparison\", pad=15)\n",
    "            plt.xlabel('Model')\n",
    "            plt.ylabel('Overall Score (0-1)')\n",
    "            plt.ylim(0, 1)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            best_model = normalized.index[0]\n",
    "            best_score = normalized['overall_score'].iloc[0]\n",
    "            print(f\"\\nBest Performing Model: {best_model.capitalize()} (Overall Score: {best_score:.4f})\")\n",
    "            \n",
    "            # --- 8. Deep Dive with Best Model -----------------------------------------\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"DEEP DIVE WITH BEST MODEL: {best_model.upper()}\")\n",
    "            print(\"=\"*50)\n",
    "\n",
    "            # Overall Sentiment Distribution\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sent_counts = df_main[f'{best_model}_sentiment'].value_counts().reindex(['positive', 'neutral', 'negative'])\n",
    "            sns.barplot(x=sent_counts.index, y=sent_counts.values, palette=['green', 'gray', 'red'], hue=sent_counts.index, legend=False)\n",
    "            for i, v in enumerate(sent_counts):\n",
    "                if not pd.isna(v):\n",
    "                    pct = f\"{v/len(df_main):.1%}\"\n",
    "                    plt.text(i, v + 5, f\"{int(v)} ({pct})\", ha='center')\n",
    "            plt.title(f\"Overall Sentiment Distribution ({best_model.capitalize()})\", pad=15)\n",
    "            plt.xlabel('Sentiment')\n",
    "            plt.ylabel('Count')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Sentiment by Topic\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sent_topic = pd.crosstab(df_main['dominant_topic_name'], df_main[f'{best_model}_sentiment'], normalize='index') * 100\n",
    "            sent_topic.plot(kind='bar', stacked=True, width=0.8, color={'positive': 'green', 'neutral': 'gray', 'negative': 'red'}, ax=plt.gca())\n",
    "            plt.title(f\"Sentiment Distribution by Topic ({best_model.capitalize()})\", pad=15)\n",
    "            plt.xlabel('Topic')\n",
    "            plt.ylabel('Percentage')\n",
    "            plt.legend(title='Sentiment', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Top Words by Sentiment\n",
    "            def get_top_words_by_sentiment(sentiment, model, n=15):\n",
    "                \"\"\"Gets top words for a specific sentiment and model from the dataset.\"\"\"\n",
    "                sent_comments = df_main[df_main[f'{model}_sentiment'] == sentiment]['tokens']\n",
    "                if sent_comments.empty or all(not comment for comment in sent_comments):\n",
    "                    return pd.DataFrame({'word': [], 'count': []})\n",
    "                \n",
    "                vec = CountVectorizer(tokenizer=lambda x: x, preprocessor=lambda x: x, token_pattern=None)\n",
    "                word_counts = vec.fit_transform(sent_comments).sum(axis=0).A1\n",
    "                vocab = vec.get_feature_names_out()\n",
    "                top_words = pd.DataFrame({'word': vocab, 'count': word_counts}).sort_values('count', ascending=False).head(n)\n",
    "                return top_words\n",
    "\n",
    "            sentiments = ['positive', 'negative', 'neutral']\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "            for i, sent in enumerate(sentiments):\n",
    "                top_words = get_top_words_by_sentiment(sent, best_model)\n",
    "                palette = 'viridis' if sent == 'positive' else ('Reds_r' if sent == 'negative' else 'coolwarm')\n",
    "                sns.barplot(x='count', y='word', data=top_words, ax=axes[i], palette=palette, hue='word', legend=False)\n",
    "                axes[i].set_title(f\"Top Words in {sent.capitalize()} Comments ({best_model.capitalize()})\", pad=10)\n",
    "                axes[i].set_xlabel('Frequency')\n",
    "                axes[i].set_ylabel('Word')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # --- 9. Export Results ----------------------------------------------------\n",
    "            # Export final dataframes to CSV files for inclusion in the project appendix.\n",
    "            print(\"\\n--- Exporting Final Results ---\")\n",
    "            df_main.to_csv('sentiment_analysis_with_model_comparison.csv', index=False)\n",
    "            metrics_df.to_csv('model_performance_metrics.csv')\n",
    "            normalized.to_csv('normalized_model_scores.csv')\n",
    "            print(\"Analysis complete. Exported files:\")\n",
    "            print(\"- youtube_comments_metadata_raw.csv\")\n",
    "            print(\"- youtube_video_metadata.csv\")\n",
    "            print(\"- youtube_comments_2023.csv\")\n",
    "            print(\"- sentiment_analysis_with_model_comparison.csv\")\n",
    "            print(\"- model_performance_metrics.csv\")\n",
    "            print(\"- normalized_model_scores.csv\")\n",
    "        else:\n",
    "            print(\"Script finished without performing full analysis due to a lack of data.\")\n",
    "    else:\n",
    "        print(\"No unique comments collected. Cannot proceed with analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dbcb8fd-fc64-4c8a-be6d-0eb9671c4e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gensim\n",
      "Version: 4.3.3\n",
      "Summary: Python framework for fast Vector Space Modelling\n",
      "Home-page: https://radimrehurek.com/gensim/\n",
      "Author: Radim Rehurek\n",
      "Author-email: me@radimrehurek.com\n",
      "License: LGPL-2.1-only\n",
      "Location: c:\\users\\queen\\anaconda3\\envs\\my_data_env\\lib\\site-packages\n",
      "Requires: numpy, scipy, smart-open\n",
      "Required-by: pyLDAvis\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5628cb1-844d-40a8-88cd-3d4603df2f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7cc04-0d16-4ba9-907c-73ee9ad75b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Environment)",
   "language": "python",
   "name": "my_data_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
